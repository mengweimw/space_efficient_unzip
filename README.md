# space_efficient_unzip
解压 zip 格式升级包，高效的空间占用实现，控制在解压后升级包的1.5倍占用

# 编译方式
gcc -O2 -Wall -Wextra -o space_efficient_unzip_v3 space_efficient_unzip_v3.c -lz

# 运行方式
./space_efficient_unzip_v3 --fast --trim-mb 32 -o ./FOTA_UNZIP ./FOTA_Upgrade.zip

# 参数
用法: ./space_efficient_unzip_v3 [选项] <zip文件>
选项:
  -o, --output DIR     指定输出目录 (默认: 当前目录)
  -f, --fast           快速模式（更大缓冲、批量裁剪、减少日志）
      --no-log         关闭存储占用日志输出
      --trim-mb N      批量裁剪阈值(单位MB，默认64)
  -h, --help           显示此帮助信息

功能:
  真正的空间高效ZIP解压工具，只占用一倍空间
  基于实际压缩方法处理文件，不依赖文件名后缀
  正确处理ZIP文件中的目录结构


# Todo
1. 问题一：V3 版本的实现，对断点续解的这种场景支持的不够好；

你的理解**完全正确**，按照目前这种“空间只占1倍”的实现方案，**不可能100%实现任意时刻的断点恢复**，尤其是在解压大文件时被中断。

---

## 原因解析

### 1. 空间只占1倍的本质
- 你的工具边解压边从 zip 包头裁剪（删除已解压 entry），这样磁盘空间始终只有“原始 zip+未解条目+已解包文件”，不会膨胀到2倍。
- 但一旦**正在解流（比如大文件），突然ctrl+C中断**，.tmp_unzip 文件流头就落在压缩体（数据块）中，而不是下一个 entry 的 header 上。

### 2. ZIP 文件格式的限制
- ZIP 格式的 entry 必须靠 local file header 对齐，只有 header 才能定位下一个 entry。
- 如果流头落在一个 entry 的压缩流中间（比如大文件被截断），恢复时**根本找不到 header，也无法继续解压**，只能全盘扫描/报错。

### 3. 业界空间高效解包工具也如此
- 所有“解压时同步删除已处理条目”的工具（如 7z 的一些特殊解包模式、unzip -O 管道解流）都存在同样局限。
- 如果要支持100%断点恢复，必须**保留原始zip包和完整的已解包 entry 列表**，那空间就不再是1倍了。

---

## 结论

- **空间只占1倍的高效解包方式，只能保证断点发生在条目之间时100%恢复成功。**
- 如果断点发生在某entry（特别是大文件）解流中间，**只能丢弃该entry，或提示手动重新解包**——这是ZIP格式和空间约束的理论极限，无法规避。

---

## 方案建议

- 推荐在工具帮助文档中直接说明：“如果中断发生在大文件解流中间，恢复时可能需要手工删除 .tmp_unzip 文件并重新解压。”
- 对于极端高可靠需求，可以设计“空间2倍占用”模式，保留zip原包和已解包列表，实现100%断点恢复，但这就不是1倍空间了。

---

**你目前的实现，已经达到了空间和断点恢复能力的最佳平衡。**  
如需加入文档说明、自动检测中断点类型、或切换到“高可靠2倍空间”模式，可随时咨询！
